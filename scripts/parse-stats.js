#!/usr/bin/env node
// scripts/parse-stats.js
// Reads dist/stats.html generated by rollup-plugin-visualizer and extracts
// the embedded module map, then writes dist/stats.json and dist/stats.csv

import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

const htmlPath = path.resolve(__dirname, '..', 'dist', 'stats.html');
if (!fs.existsSync(htmlPath)) {
	console.error('dist/stats.html not found. Run npm run build:analyze first.');
	process.exit(2);
}

const html = fs.readFileSync(htmlPath, 'utf8');

// Heuristic: find the big JSON object that contains module ids like "/src/"
// We search for the first occurrence of '"/src/' or '"/node_modules/' and then
// walk backwards to find the opening '{' of the object containing it, and forward
// to the matching closing '}'. This is brittle but works for the visualizer output.

function findJsonContaining(substr) {
	const idx = html.indexOf(substr);
	if (idx === -1) return null;
	// find a '{' before idx that likely starts the object
	let start = html.lastIndexOf('{', idx);
	if (start === -1) start = 0;
	// now find the matching closing brace by simple stack
	let i = start;
	let depth = 0;
	for (; i < html.length; i++) {
		if (html[i] === '{') depth++;
		else if (html[i] === '}') {
			depth--;
			if (depth === 0) {
				// return the substring from start to i
				return html.slice(start, i + 1);
			}
		}
	}
	return null;
}

// Prefer extracting the explicit `const data = { ... };` block emitted by the visualizer
let data = null;
const anchor = 'const data =';
const anchorIdx = html.indexOf(anchor);
if (anchorIdx !== -1) {
	// find the first '{' after the anchor and match braces
	const braceStart = html.indexOf('{', anchorIdx);
	if (braceStart === -1) {
		console.error('Could not find opening brace after "const data ="');
		process.exit(2);
	}
	let i = braceStart;
	let depth = 0;
	let end = -1;
	for (; i < html.length; i++) {
		const ch = html[i];
		if (ch === '{') depth++;
		else if (ch === '}') {
			depth--;
			if (depth === 0) {
				end = i;
				break;
			}
		}
	}
	if (end === -1) {
		console.error('Could not find matching closing brace for data object');
		process.exit(2);
	}

	const jsonText = html.slice(braceStart, end + 1);
	try {
		data = JSON.parse(jsonText);
	} catch (err) {
		console.error(
			'Failed to parse data JSON from stats.html: ',
			err && err.message ? err.message : err
		);
		process.exit(2);
	}
} else {
	// Fallback: try earlier heuristic
	const jsonText =
		findJsonContaining('"/src/') ||
		findJsonContaining('"/node_modules/') ||
		findJsonContaining('"id":');
	if (!jsonText) {
		console.error('Could not locate JSON data in stats.html');
		process.exit(2);
	}
	try {
		data = JSON.parse(jsonText);
	} catch (err) {
		console.error('Failed to parse fallback JSON: ', err && err.message ? err.message : err);
		process.exit(2);
	}
}

// Prefer to build the module list from the visualizer's explicit maps when present
let list = [];
if (data && data.nodeMetas && data.nodeParts) {
	// Build a map of id -> aggregated info so we can merge part-level entries too
	const map = new Map();

	// First, aggregate sizes from nodeMetas by summing referenced nodeParts when needed
	for (const [metaUid, meta] of Object.entries(data.nodeMetas)) {
		const id = meta.id || meta.name || metaUid;
		let size = Number(meta.renderedLength || meta.gzipLength || meta.brotliLength || 0) || 0;

		if ((!size || size === 0) && meta.moduleParts) {
			for (const partUid of Object.values(meta.moduleParts)) {
				const part = data.nodeParts[partUid];
				if (part) {
					size += Number(part.renderedLength || part.gzipLength || part.brotliLength || 0) || 0;
				}
			}
		}

		map.set(id, { id, size, raw: meta });
	}

	// Also include nodeParts that carry their own id (these are often the actual chunk/entry files)
	for (const part of Object.values(data.nodeParts)) {
		if (!part || !part.id) continue;
		const id = part.id;
		const size = Number(part.renderedLength || part.gzipLength || part.brotliLength || 0) || 0;

		const existing = map.get(id);
		// Prefer the larger observed size (some entries appear both as a meta and as a part)
		if (!existing) map.set(id, { id, size, raw: part });
		else if (size > existing.size) map.set(id, { id, size, raw: part });
	}

	list = Array.from(map.values());
} else {
	// The older/fallback heuristic: flatten any objects with an `id` field
	const entries = [];
	function collect(obj) {
		if (!obj || typeof obj !== 'object') return;
		for (const k of Object.keys(obj)) {
			const v = obj[k];
			if (v && typeof v === 'object' && v.id) entries.push({ id: v.id, info: v });
			else collect(v);
		}
	}
	collect(data);
	const map = new Map();
	for (const e of entries) {
		const id = e.id;
		if (!map.has(id)) map.set(id, e.info);
	}
	for (const [id, info] of map.entries()) {
		let size = 0;
		// try to derive size from moduleParts by locating referenced parts on disk as a last resort
		if (info.moduleParts) {
			for (const part of Object.values(info.moduleParts)) {
				const maybe = path.resolve(__dirname, '..', '.svelte-kit', 'output', 'client', part);
				try {
					if (fs.existsSync(maybe)) {
						const stat = fs.statSync(maybe);
						size += stat.size;
					}
				} catch {
					// ignore
				}
			}
		}
		list.push({ id, size, raw: info });
	}
}

// Sort by size (descending)
list.sort((a, b) => b.size - a.size);

// Write JSON and CSV
const outJson = path.resolve(__dirname, '..', 'dist', 'stats.json');
fs.writeFileSync(outJson, JSON.stringify(list, null, 2));

const outCsv = path.resolve(__dirname, '..', 'dist', 'stats.csv');
const csv = ['id,size'].concat(list.map((l) => `${JSON.stringify(l.id)},${l.size}`)).join('\n');
fs.writeFileSync(outCsv, csv);

console.log('Wrote', outJson, 'and', outCsv);

// Print top 20
console.log('\nTop 20 modules (by estimated bytes):');
list.slice(0, 20).forEach((l, i) => {
	console.log(`${i + 1}. ${l.id} â€” ${l.size} bytes`);
});

process.exit(0);
